## Домашнее задание 1

- hw_1_styles.py - генератор стилей (результат - файлы txt)
- hw_1_avatars.py - генератор аватаров (результат - png изображения)
- hw_1_part_2.ipynb - автоэнкодер (классификация изображений с проливами)

Для генератора стилей использован styles.py, для генератора аватаров - папка avatars. Генератор аватаров на основе MLE генерирует значительно похожие друг на друга изображения (разница на уровне пикселей), генератор на основе формулы Байеса генерирует шум. Для классификации изображений с проливами использовался датасет из репозитория. Лучший результат, которого удалось достичь - 

True Positive Rate: 0.9457\
True Negative Rate: 0.8049

Команда для вывода графиков функции потерь при обучении: ```tensorboard --logdir runs```

Остальные результаты представлены в таблице:

| Количество линейных слоев | Перевод в оттенки серого | Dropout | Подбор treshold            | AUC    | TPR    | TNR    |
|---------------------------|--------------------------|---------|----------------------------|--------|--------|--------|
| 5                         | Нет                     | Нет     | Среднее по лоссам + 3 сигмы | -      | 0.7752 | 0.8368 |
| 4                         | Нет                     | Нет     | Через ROC/AUC              | 0.8975 | 0.7674 | 0.8750 |
| 4                         | Да                      | 0.2     | Через ROC/AUC              | 0.9475 | 0.9457 | 0.8049 |
| 3                         | Да                      | Нет     | Через ROC/AUC              | 0.9112 | 0.7054 | 0.9683 |

Результат получен при переводе изображений в оттенки серого, 4 эпохах обучения и изменения модели автоэнкодера. Предыдущие попытки зафиксированы в предыдущих коммитах. 

Вывод: необходимо еще поработать над конфигурацией модели (она склонна к переобучению). Однако по сравнению с бейслайном есть прогресс, связываю это с использованием grayscale и более тщательным подходом к выбору порога, а также изменением конфигурации модели (более широкий bottleneck)